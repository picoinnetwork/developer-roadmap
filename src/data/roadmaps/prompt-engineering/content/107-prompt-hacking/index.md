# Prompt Hacking

Prompt hacking refers to techniques used to manipulate or exploit AI language models by carefully crafting input prompts. This practice aims to bypass the model's intended constraints or elicit unintended responses. Common methods include injection attacks, where malicious instructions are embedded within seemingly innocent prompts, and prompt leaking, which attempts to extract sensitive information from the model's training data.

Visit the following resources to learn more:

- [@article@Prompt Hacking](https://learnprompting.org/docs/prompt_hacking/intro)
- [@feed@Explore top posts about Security](https://app.daily.dev/tags/security?ref=roadmapsh)

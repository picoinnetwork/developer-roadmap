# Jailbreaking

Jailbreaking bypasses AI models' ethical constraints and safety measures. Attackers use carefully crafted prompts to manipulate models into generating harmful, biased, or inappropriate content, potentially leading to misuse of AI systems.

Visit the following resources to learn more:

- [@article@Jailbreaking](https://learnprompting.org/docs/prompt_hacking/jailbreaking)
- [@opensource@Jailbreaking](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/guides/prompts-adversarial.md#jailbreaking)
# Prompt Injection

Prompt injection exploits vulnerabilities in AI systems by inserting malicious instructions into user inputs. Attackers manipulate the model's behavior, potentially bypassing safeguards or extracting sensitive information. This technique poses security risks for AI-powered applications.

Visit the following resources to learn more:

- [@article@Prompt Injection](https://learnprompting.org/docs/prompt_hacking/injection)
- [@article@IBM Article](https://www.ibm.com/topics/prompt-injection)
